from collections import Counter
from datetime import datetime
from sklearn.feature_extraction.text import CountVectorizer


def ff_system_call_counts(tree):
    c = Counter()
    for all_section in tree.iter('all_section'):
        for call in all_section.iter():
            c[call.tag] += 1
    return c


def ff_program_metadata(tree):
    total_runtime = 0
    num_threads = 0
    num_procs = 0
    metadata = Counter()
    total_size = 0
    valid_file_sizes = 0
    invalid_file_sizes = 0

    for process in tree.iter('process'):

        start = datetime.strptime(process.get('starttime'), '%M:%S.%f')
        end = datetime.strptime(process.get('terminationtime'), '%M:%S.%f')
        total = end - start
        total_runtime += total.total_seconds()

        metadata[process.get('terminationreason')] += 1
        metadata[process.get('startreason')] += 1
        metadata[process.get('executionstatus')] += 1
        #         metadata[process.get('applicationtype')] += 1
        metadata[process.get('username')] += 1

        fsize = int(process.get('filesize'))
        if fsize != -1:
            total_size += fsize
            valid_file_sizes += 1
        elif fsize == -1:
            invalid_file_sizes += 1

        num_procs += 1
        threads = process.findall('thread')
        num_threads += len(threads)

    # Normalize metadata counts relative to num processes
    for k, v in metadata.iteritems():
        metadata[k] = float(v) / num_procs

    metadata['num_procs'] = num_procs
    metadata['total_runtime'] = total_runtime
    metadata['avg_fsize'] = float(total_size) / valid_file_sizes
    metadata['avg_threads_per_proc'] = float(num_threads) / num_procs
    metadata['num_invalid_fsizes'] = invalid_file_sizes

    return metadata

def ff_ngrams(tree):
    call_list = []
    for all_section in tree.iter('all_section'):
        for call in all_section.iter():
            call_list.append(call.tag)
    cv = CountVectorizer(ngram_range=(1,4))
    ngrams = cv.fit_transform(call_list)
    print ngrams



