# This file provides starter code for extracting features from the xml files and
# for doing some learning.
#
# The basic set-up:
# ----------------
# main() will run code to extract features, learn, and make predictions.
#
# extract_feats() is called by main(), and it will iterate through the
# train/test directories and parse each xml file into an xml.etree.ElementTree,
# which is a standard python object used to represent an xml file in memory.
# (More information about xml.etree.ElementTree objects can be found here:
# http://docs.python.org/2/library/xml.etree.elementtree.html
# and here: http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/)
# It will then use a series of "feature-functions" that you will write/modify
# in order to extract dictionaries of features from each ElementTree object.
# Finally, it will produce an N x D sparse design matrix containing the union
# of the features contained in the dictionaries produced by your "feature-functions."
# This matrix can then be plugged into your learning algorithm.
#
# The learning and prediction parts of main() are largely left to you, though
# it does contain code that randomly picks class-specific weights and predicts
# the class with the weights that give the highest score. If your prediction
# algorithm involves class-specific weights, you should, of course, learn
# these class-specific weights in a more intelligent way.
#
# Feature-functions:
# --------------------
# "feature-functions" are functions that take an ElementTree object representing
# an xml file (which contains, among other things, the sequence of system calls a
# piece of potential malware has made), and returns a dictionary mapping feature names to
# their respective numeric values.
# For instance, a simple feature-function might map a system call history to the
# dictionary {'first_call-load_image': 1}. This is a boolean feature indicating
# whether the first system call made by the executable was 'load_image'.
# Real-valued or count-based features can of course also be defined in this way.
# Because this feature-function will be run over ElementTree objects for each
# software execution history instance, we will have the (different)
# feature values of this feature for each history, and these values will make up
# one of the columns in our final design matrix.
# Of course, multiple features can be defined within a single dictionary, and in
# the end all the dictionaries returned by feature functions (for a particular
# training example) will be unioned, so we can collect all the feature values
# associated with that particular instance.
#
# Two example feature-functions, first_last_system_call_feats() and
# system_call_count_feats(), are defined below.
# The first of these functions indicates what the first and last system-calls
# made by an executable are, and the second records the total number of system
# calls made by an executable.
#
# What you need to do:
# --------------------
# 1. Write new feature-functions (or modify the example feature-functions) to
# extract useful features for this prediction task.
# 2. Implement an algorithm to learn from the design matrix produced, and to
# make predictions on unseen data. Naive code for these two steps is provided
# below, and marked by TODOs.
#
# Computational Caveat
# --------------------
# Because the biggest of any of the xml files is only around 35MB, the code below
# will parse an entire xml file and store it in memory, compute features, and
# then get rid of it before parsing the next one. Storing the biggest of the files
# in memory should require at most 200MB or so, which should be no problem for
# reasonably modern laptops. If this is too much, however, you can lower the
# memory requirement by using ElementTree.iterparse(), which does parsing in
# a streaming way. See http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/
# for an example.

from __future__ import print_function
import os
try:
    import xml.etree.cElementTree as ET
except ImportError:
    import xml.etree.ElementTree as ET
import numpy as np
from scipy import sparse

from glob import glob
import pickle
import random

import util

import feature_functions
import training
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import train_test_split


def extract_feats(ffs, files, global_feat_dict=None, debug=False):
    """
    arguments:
      ffs are a list of feature-functions.
      direc is a directory containing xml files (expected to be train or test).
      global_feat_dict is a dictionary mapping feature_names to column-numbers; it
      should only be provided when extracting features from test data, so that 
      the columns of the test matrix align correctly.

    returns: 
      a sparse design matrix, a dict mapping features to column-numbers,
      a vector of target classes, and a list of system-call-history ids in order 
      of their rows in the design matrix.
      
      Note: the vector of target classes returned will contain the true indices of the
      target classes on the training data, but will contain only -1's on the test
      data
    """
    fds = []  # list of feature dicts
    classes = []
    ids = [] 
    for (d, datafile) in enumerate(files):
        if (d % 100 == 0) and debug:
            print (d)
        # extract id and true class (if available) from filename
        id_str_and_file, clazz = datafile.split('.')[:2]
        _,id_str = id_str_and_file.split('/')
        ids.append(id_str)
        # add target class if this is training data
        try:
            classes.append(util.malware_classes.index(clazz))
        except ValueError:
            # we should only fail to find the label in our list of malware classes
            # if this is test data, which always has an "X" label
            assert clazz == "X"
            classes.append(-1)

        rowfd = {}
        # parse file as an xml document
        filepath = os.path.abspath(datafile)
        tree = ET.parse(filepath)
        # accumulate features
        [rowfd.update(ff(tree)) for ff in ffs]
        fds.append(rowfd)
        
    X, feat_dict = make_design_mat(fds, global_feat_dict)
    return X, feat_dict, np.array(classes), ids


def make_design_mat(fds, global_feat_dict=None):
    """
    arguments:
      fds is a list of feature dicts (one for each row).
      global_feat_dict is a dictionary mapping feature_names to column-numbers; it
      should only be provided when extracting features from test data, so that 
      the columns of the test matrix align correctly.
       
    returns: 
        a sparse NxD design matrix, where N == len(fds) and D is the number of
        the union of features defined in any of the fds 
    """
    if global_feat_dict is None:
        all_feats = set()
        [all_feats.update(fd.keys()) for fd in fds]
        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])
    else:
        feat_dict = global_feat_dict
        
    cols = []
    rows = []
    data = []        
    for i in xrange(len(fds)):
        temp_cols = []
        temp_data = []
        for feat, val in fds[i].iteritems():
            try:
                # update temp_cols iff update temp_data
                temp_cols.append(feat_dict[feat])
                temp_data.append(val)
            except KeyError as ex:
                if global_feat_dict is not None:
                    pass  # new feature in test data; nbd
                else:
                    raise ex

        # all fd's features in the same row
        k = len(temp_cols)
        cols.extend(temp_cols)
        data.extend(temp_data)
        rows.extend([i]*k)

    assert len(cols) == len(rows) and len(rows) == len(data)

    X = sparse.csr_matrix((np.array(data),
                   (np.array(rows), np.array(cols))),
                   shape=(len(fds), len(feat_dict)))
    return X, feat_dict


def extract_features():
    train_files = glob('train/*.xml')
    test_files = glob('test/*.xml')
    training_num_files = 3086

    ffs = [getattr(feature_functions, func) for func in dir(feature_functions) if func[0:2] == 'ff']
    file_sample = random.sample(train_files, training_num_files)

    # extract features
    if os.path.exists('feature_extract.pkl'):
        print('loading existing feature extracts')
        X, global_feat_dict, t_train, train_ids = pickle.load(open('feature_extract.pkl', 'r'))
    else:
        print("extracting training features...")
        X, global_feat_dict, t_train, train_ids = extract_feats(ffs, file_sample)
        print("done extracting training features")
        with open('feature_extract.pkl', 'w') as f:
            pickle.dump((X, global_feat_dict, t_train, train_ids), f)

    return X, t_train


# The following function does the feature extraction, learning, and prediction
def train(outputfile='mypredictions.csv'):
    train_files = glob('train/*.xml')
    test_files = glob('test/*.xml')
    training_num_files = 3086
    force_extract = False
    debug = True

    ffs = [getattr(feature_functions, func) for func in dir(feature_functions) if func[0:2] == 'ff']
    file_sample = random.sample(train_files, training_num_files)

    # extract features
    if os.path.exists('feature_extract.pkl') and not force_extract:
        print('loading existing feature extracts')
        X, global_feat_dict, t_train, train_ids = pickle.load(open('feature_extract.pkl', 'r'))
    else:
        print ("extracting training features...")
        X, global_feat_dict, t_train, train_ids = extract_feats(ffs, file_sample, debug=debug)
        print ("done extracting training features")
        with open('feature_extract.pkl', 'w') as f:
            pickle.dump((X, global_feat_dict, t_train, train_ids), f)

    X_train, X_test, y_train, y_test = train_test_split(X, t_train, test_size=0.2)
    print ("learning...")
    clf = RandomForestClassifier(class_weight='balanced', n_estimators=356, min_samples_split=3, min_samples_leaf=1, max_features=73)
    training.sklearn_train(clf, X_train, y_train)

    print ("done learning")

    # preds = clf.predict(X_test)
    #
    # corrects = 0.0
    # for a, b in zip(preds, y_test):
    #     if a == b:
    #         corrects += 1
    # print
    # print (corrects / len(preds))

    print ("extracting test features...")
    X_test,_,t_ignore,test_ids = extract_feats(ffs, test_files, global_feat_dict=global_feat_dict, debug=debug)
    print ("done extracting test features")
    #
    # # TODO make predictions on text data and write them out
    # print "making predictions..."
    preds = clf.predict(X_test)
    print ("done making predictions")


    print ("writing predictions...")
    util.write_predictions(preds, test_ids, outputfile)
    print ("done!")


def test():
    pass


if __name__ == "__main__":
    train()
